{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26046,"status":"ok","timestamp":1697042599010,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"qollEV2dQEOd","outputId":"07309da4-c5fe-4863-e2a4-10429adfde2c"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16296,"status":"ok","timestamp":1697042615299,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"lu5SHhJhQBJk"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn import CrossEntropyLoss\n","from torchsummary import summary\n","from torch.optim import Adam\n","from torch import cuda\n","\n","import torch.nn as nn\n","import torch\n","\n","import pandas as pd\n","import numpy as np\n","\n","import joblib\n","import nltk\n","\n","import re\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5147,"status":"ok","timestamp":1697043425102,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"3FENRzmSskM2"},"outputs":[],"source":["train_dir = \"path/to/data\"\n","train_prefix = \"train-prefix\"\n","\n","eval_dir = \"path/to/data\"\n","eval_prefix = \"eval-prefix\"\n","\n","results_dir = \"path/to/results\"\n","\n","train_dataset = pd.read_csv(f'{train_dir}/{train_prefix}-train-features.csv')\n","dev_dataset = pd.read_csv(f'{eval_dir}/{eval_prefix}-dev-features.csv')\n","test_dataset = pd.read_csv(f'{eval_dir}/{eval_prefix}-test-features.csv')"]},{"cell_type":"markdown","metadata":{"id":"iEJKjoegQBJo"},"source":["### Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1697043425103,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"hn42XiMPQBJr","outputId":"9302ee9b-3246-4bab-a9d8-14f6617e5f78"},"outputs":[],"source":["train_feature_based = train_dataset.drop(columns=['label','text','POS-tagged','seq_len'])\n","train_feature_based.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1697043425103,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"qn1vTjfaQBJs","outputId":"518c1167-e501-4ee0-8d82-7407a66305fb"},"outputs":[],"source":["print(train_feature_based.corr(numeric_only=True)['label_bool'].sort_values(ascending=False)[1:])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":620},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1696925489035,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"yNIecJ5wtEOB","outputId":"02bf96b8-bd37-440e-9824-224fb0f44059"},"outputs":[],"source":["dev_feature_based = dev_dataset.drop(columns=['label','text','POS-tagged','seq_len'])\n","print(dev_feature_based.corr(numeric_only=True)['label_bool'].sort_values(ascending=False)[1:])\n","dev_feature_based.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":620},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1696925490792,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"LnAU35Mplmy-","outputId":"fd926353-7009-4de6-e019-a1dcb7c66853"},"outputs":[],"source":["test_feature_based = test_dataset.drop(columns=['label','text','POS-tagged','seq_len'])\n","print(test_feature_based.corr(numeric_only=True)['label_bool'].sort_values(ascending=False)[1:])\n","test_feature_based.head()"]},{"cell_type":"markdown","metadata":{"id":"GKbLAVs8QBJt"},"source":["### Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X95dis-tQBJt"},"outputs":[],"source":["def train_and_evaluate(input, labels, model, random_state=42):\n","    X_train, X_test, y_train, y_test = train_test_split(input, labels, test_size=0.2, random_state=random_state)\n","    # print number of test samples per class\n","    # print(\"Number of test samples per class: \", np.bincount(y_train))\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    return accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred), confusion_matrix(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFxDIJDfs1K9"},"outputs":[],"source":["def train_and_evaluate_split(X_train, X_test, y_train, y_test, model):\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    return accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred), confusion_matrix(y_test, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"8zoPaXyJQBJu"},"source":["#### Multinomial Naive Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rSdh-PVQBJw"},"outputs":[],"source":["method = \"lr\"\n","classifier = MultinomialNB() if method == \"mb\" else LogisticRegression()\n","evaluation = \"test\"\n","\n","test = dev_feature_based if evaluation == \"dev\" else test_feature_based\n","\n","X_train, X_test, y_train, y_test = train_feature_based.drop(columns=['label_bool']), test.drop(columns=['label_bool']), train_feature_based['label_bool'], test['label_bool']\n","\n","metrics = train_and_evaluate_split(X_train, X_test, y_train, y_test, classifier)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1696871804713,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"yfcK0m7rQBJy","outputId":"804bdb1b-e4c9-4506-814e-f28fa50bf145"},"outputs":[],"source":["print('Accuracy: ', metrics[0])\n","print('Precision: ', metrics[1])\n","print('Recall: ', metrics[2])\n","print('F1: ', metrics[3])\n","print('Confusion Matrix: ', metrics[4])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":516,"status":"ok","timestamp":1688987851238,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"i2vwDNE7QBJz","outputId":"55b65cf1-4d52-4180-e9a4-1be5b00439fb"},"outputs":[],"source":["# print 5 missclassified samples\n","X_train, X_test, y_train, y_test = train_test_split(feature_based.drop(columns=['label_bool']), dataset['label_bool'], test_size=0.2, random_state=42)\n","model = MultinomialNB()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","missclassified = np.where(y_test != y_pred)[0][:10]\n","# print the missclassified samples\n","for i in missclassified:\n","    print('Text: ', dataset['text'][i])\n","    print('Label: ', dataset['label'][i])\n","    print('------------------------')"]},{"cell_type":"markdown","metadata":{"id":"dp361YZJQBJ1"},"source":["#### N-Gram Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bELaqVkjQBJ1","outputId":"3716b461-ccc4-43c4-9f57-167479bff1e9"},"outputs":[],"source":["filtered_dataset = dataset.copy()\n","filtered_dataset.text = filtered_dataset.text.apply(lambda x: x.lower())\n","filtered_dataset.text = filtered_dataset.text.apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n","filtered_dataset.text = filtered_dataset.text.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n","filtered_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gX8nd2iUQBJ2"},"outputs":[],"source":["def get_ngrams(train_df, eval_df, ngram_range=(1, 6), max_features=10000, show=False):\n","    ngrams = []\n","\n","    for n in range(1, ngram_range[1] + 1):\n","        vectorizer = CountVectorizer(ngram_range=(1, n), max_features=max_features)\n","        train_ngram = vectorizer.fit_transform(train_df['text'])\n","        eval_ngram = vectorizer.transform(eval_df['text'])\n","        if show:\n","            print(f\"{n}-grams train shape:\", train_ngram.shape)\n","        ngrams.append((train_ngram, eval_ngram))\n","\n","    return ngrams\n","\n","def get_ngrams_results(ngrams, train_labels, eval_labels, show=False, max_features=10000, classifier=MultinomialNB()):\n","    results = []\n","    for i in range(len(ngrams)):\n","        metrics = train_and_evaluate_split(ngrams[i][0], ngrams[i][1], train_labels, eval_labels, classifier)\n","        metrics = [metrics[0], metrics[1], metrics[2], metrics[3], metrics[4][0][0], metrics[4][0][1], metrics[4][1][0], metrics[4][1][1], i+1]\n","        results.append(metrics)\n","\n","    return pd.DataFrame(results, columns=['accuracy', 'precision', 'recall', 'f1', 'true_positive', 'false_positive', 'false_negative', 'true_negative', 'max_ngram'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZY4tJlAEA4r"},"outputs":[],"source":["method = \"mb\"\n","classifier = MultinomialNB() if method == \"mb\" else LogisticRegression()\n","evaluation = \"test\"\n","eval_dataset = dev_dataset if evaluation == \"dev\" else test_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141875,"status":"ok","timestamp":1696925995620,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"563EpdWlD4Im","outputId":"9808912c-f204-4403-a4a6-9c063855a306"},"outputs":[],"source":["ngrams = get_ngrams(train_dataset, eval_dataset, show=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1696925995958,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"mr5TwlazQBJ3","outputId":"784bf834-8231-4ff5-8db1-31ec8e34acb0"},"outputs":[],"source":["df = get_ngrams_results(ngrams, train_dataset['label_bool'], eval_dataset['label_bool'], show=True, max_features=10000, classifier=classifier)\n","df.head(10)"]},{"cell_type":"markdown","metadata":{"id":"2BpiMKeNj-ez"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rUkGocsCQBJ4"},"outputs":[],"source":["df.to_csv(f'{results_dir}/{train_prefix}-{eval_prefix}-{evaluation}-ngram-{method}-results.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"Jq_JnCrUQBJ4"},"source":["#### N-Gram Model with POS Tagging"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HEwGYRXqQBJ5"},"outputs":[],"source":["pt_pos_tagger = joblib.load('/content/drive/MyDrive/PTvsBR/POS_tagger_brill.pkl')\n","\n","def tag_sentence(sentence):\n","    sentence = sentence.lower()\n","    sentence = nltk.word_tokenize(sentence, language='portuguese')\n","    return pt_pos_tagger.tag(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":551},"executionInfo":{"elapsed":1346,"status":"ok","timestamp":1696925999199,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"X_2URGILQBJ6","outputId":"97b6414c-bd19-4ccf-a345-ae17405e3713"},"outputs":[],"source":["train_pos_tagged = train_dataset.copy()\n","train_pos_tagged['text'] = train_pos_tagged['POS-tagged'].apply(lambda x: ' '.join(x.split(\"@@@\")))\n","train_pos_tagged.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":586},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1696925999212,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"Ef2f59XVBsEG","outputId":"8bf234d1-e772-46c3-8b85-afdcdd8f2db1"},"outputs":[],"source":["dev_pos_tagged = dev_dataset.copy()\n","dev_pos_tagged['text'] = dev_pos_tagged['POS-tagged'].apply(lambda x: ' '.join(x.split(\"@@@\")))\n","dev_pos_tagged.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":551},"executionInfo":{"elapsed":480,"status":"ok","timestamp":1696925999686,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"_gjvnIcekZRL","outputId":"b9f0ca62-01e1-4157-ca89-636937a2b2ef"},"outputs":[],"source":["test_pos_tagged = test_dataset.copy()\n","test_pos_tagged['text'] = test_pos_tagged['POS-tagged'].apply(lambda x: ' '.join(x.split(\"@@@\")))\n","test_pos_tagged.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O6mJxyfuQBJ6"},"outputs":[],"source":["method = \"mb\"\n","classifier = MultinomialNB() if method == \"mb\" else LogisticRegression()\n","evaluation = \"test\"\n","eval_pos_tagged = dev_pos_tagged if evaluation == \"dev\" else test_pos_tagged"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"795KcWgKGpoR"},"outputs":[],"source":["pos_tagged_ngrams = get_ngrams(train_pos_tagged, eval_pos_tagged, show=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hSxjZYl8GhUh","outputId":"eb56d142-236d-48bd-9fa4-7b34d99d13f8"},"outputs":[],"source":["df = get_ngrams_results(pos_tagged_ngrams, train_pos_tagged['label_bool'], eval_pos_tagged['label_bool'], show=True, max_features=10000, classifier=classifier)\n","df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKj8l-UxQBJ7"},"outputs":[],"source":["df.to_csv(f'{results_dir}/{train_prefix}-{eval_prefix}-{evaluation}-pos-ngram-{method}-results.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"FMo9j3Bh4LwT"},"source":["\n","#### Adaptive Version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3hjtSS_4Ovv"},"outputs":[],"source":["ADAPTIVE_TRAIN_ITERATIONS = 4\n","\n","def adaptive_train(X_train, y_train, model, size, show=False):\n","  model.partial_fit(X_train[:size], y_train[:size], classes=[0, 1])\n","  # Iterate over subsets using groupby\n","  a = size // 10\n","  for i in range(1, len(y_train) // size):\n","    if show:\n","      print(f\"Training subset {i} ({i*size}:{(i+1)*size}/{len(y_train)})...\")\n","    X_subset, y_subset = list(X_train[i*size:(i+1)*size].toarray()), list(y_train[i*size:(i+1)*size])\n","    for iteration in range(ADAPTIVE_TRAIN_ITERATIONS):\n","      if len(y_subset) == 0:\n","        if show:\n","          print(\"All elements processed\")\n","        break\n","      predictions = model.predict_proba(X_subset)\n","      indexes = top_indexes(predictions, lambda x: abs(x[0] - x[1]), a)\n","      if len(indexes) == 0:\n","        if show:\n","          print(\"Not enough confidence.\")\n","        break\n","      X_removed = [X_subset.pop(index) for index in reversed(indexes)]\n","      y_removed = [y_subset.pop(index) for index in reversed(indexes)]\n","      model.partial_fit(X_removed, y_removed)\n","  return model\n","\n","\n","def top_indexes(subset, criteria, n):\n","    subset = [(i, item) for i, item in enumerate(subset)]\n","    subset = sorted(subset, key=lambda x: criteria(x[1]), reverse=True)\n","    return list(sorted([i for i, _ in subset[:n]]))\n","\n","\n","def evaluate(X_test, y_test, model):\n","  y_pred = model.predict(X_test)\n","  return accuracy_score(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred), confusion_matrix(y_test, y_pred)\n","\n","def get_model(ngrams, y, size=512, model_key=\"mb\", show=False):\n","  models = []\n","  for i in range(len(ngrams)):\n","    if show:\n","      print(f\"Training ngrams with max size {i+1}\")\n","    new_model = adaptive_train(ngrams[i][0], y, MultinomialNB() if model_key==\"mb\" else LogisticRegression(), size, show=show)\n","    models.append(new_model)\n","\n","  return models\n","\n","\n","def adaptive_results(ngrams, y, models):\n","  results = []\n","  for i in range(len(ngrams)):\n","    metrics = evaluate(ngrams[i][1], y, models[i])\n","    metrics = [metrics[0], metrics[1], metrics[2], metrics[3], metrics[4][0][0], metrics[4][0][1], metrics[4][1][0], metrics[4][1][1], i+1]\n","    results.append(metrics)\n","\n","  return pd.DataFrame(results, columns=['accuracy', 'precision', 'recall', 'f1', 'true_positive', 'false_positive', 'false_negative', 'true_negative', 'max_ngram'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432304,"status":"ok","timestamp":1696765949998,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"Sx-jBVSyRW3M","outputId":"4204fa94-3ed6-4f3b-a46b-7f22d9a76a94"},"outputs":[],"source":["from sklearn.utils import shuffle\n","\n","evaluation = \"dev\"\n","eval_dataset = dev_dataset if evaluation==\"dev\" else test_dataset\n","train_shuffled = shuffle(train_dataset.copy(), random_state=42)\n","train_shuffled.reset_index(inplace=True, drop=True)\n","ngrams = get_ngrams(train_shuffled, eval_dataset, show=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVgmUDAa6d_j"},"outputs":[],"source":["method = \"mb\"\n","size = 1024 * 16\n","\n","saved_models = []\n","\n","for i in range(10):\n","  models = get_model(ngrams, train_shuffled['label_bool'], size=size, show=False)\n","  saved_models.append(models)\n","  df = adaptive_results(ngrams, eval_dataset['label_bool'], models)\n","  df.to_csv(f'{results_dir}/{train_prefix}-{eval_prefix}-{evaluation}-adaptive-{size}-splits-ngram-{method}-results.csv', index=False)\n","  if size <= 16:\n","    break\n","  size = size // 2"]},{"cell_type":"markdown","metadata":{"id":"npbERUdhBpQW"},"source":["#### Adaptive w/ POS Tagging"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":538112,"status":"ok","timestamp":1696768036855,"user":{"displayName":"David Preda","userId":"09246923591485313118"},"user_tz":-60},"id":"XGyNWDk-Bhtk","outputId":"4c097bee-78a1-48b3-a13b-6f563b902662"},"outputs":[],"source":["from sklearn.utils import shuffle\n","\n","evaluation = \"dev\"\n","eval_pos_tagged = dev_pos_tagged if evaluation==\"dev\" else test_pos_tagged\n","train_shuffled = shuffle(train_pos_tagged.copy(), random_state=42)\n","train_shuffled.reset_index(inplace=True, drop=True)\n","pos_ngrams = get_ngrams(train_shuffled, eval_pos_tagged, show=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egP97KvUBjn7"},"outputs":[],"source":["method = \"mb\"\n","size = 1024 * 16\n","\n","for i in range(10):\n","  models = get_model(pos_ngrams, train_shuffled['label_bool'], size=size, show=False)\n","  saved_models.append(models)\n","  df = adaptive_results(pos_ngrams, eval_pos_tagged['label_bool'], models)\n","  df.to_csv(f'{results_dir}/{train_prefix}-{eval_prefix}-{evaluation}-pos-adaptive-{size}-splits-ngram-{method}-results.csv', index=False)\n","  if size <= 16:\n","    break\n","  size = size // 2"]},{"cell_type":"markdown","metadata":{"id":"F2hck2GzQBJ7"},"source":["### N-Grams with Neural Networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFlvUjMOQBJ7"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(Net, self).__init__()\n","        self.l1 = nn.Sequential(\n","            nn.Linear(input_size, hidden_size),\n","            nn.BatchNorm1d(hidden_size),\n","            nn.ReLU()\n","        )\n","        self.l2 = nn.Sequential(\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.BatchNorm1d(hidden_size),\n","            nn.ReLU()\n","        )\n","        self.l3 = nn.Sequential(\n","            nn.Linear(hidden_size, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        out = self.l1(x)\n","        out = self.l2(out)\n","        out = self.l3(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aea05WJTQBJ8","outputId":"bc7efed2-ffff-4647-9fe1-99d8d2f20ae5"},"outputs":[],"source":["model = Net(1000, 100, 1)\n","summary(model, (1000,))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnFnLEsWQBJ8"},"outputs":[],"source":["learning_rate = 1e-3\n","batch_size = 256\n","criterion = nn.BCELoss()\n","epochs = 5\n","device = 'cuda' if cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAojysBqQBJ9"},"outputs":[],"source":["def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10, device='cpu'):\n","    train_loss_history = []\n","    val_loss_history = []\n","    for epoch in range(epochs):\n","        model.train()\n","        number_of_batches = 0\n","        train_loss_history.append(0)\n","\n","        for X, y in train_loader:\n","            X = X.to(device)\n","            y = y.to(device).unsqueeze(1).to(torch.float32)\n","            optimizer.zero_grad()\n","            y_pred = model(X.float())\n","            loss = criterion(y_pred, y)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss_history[-1] += loss.item()\n","            number_of_batches += 1\n","\n","        train_loss_history[-1] /= number_of_batches\n","\n","        model.eval()\n","        number_of_batches = 0\n","        val_loss_history.append(0)\n","        with torch.no_grad():\n","            for X, y in val_loader:\n","                X = X.to(device)\n","                y = y.to(device).unsqueeze(1).to(torch.float32)\n","                y_pred = model(X.float())\n","                loss = criterion(y_pred, y)\n","                val_loss_history[-1] += loss.item()\n","                number_of_batches += 1\n","\n","        val_loss_history[-1] /= number_of_batches\n","        print('Epoch: {} - Train Loss: {:.6f} - Val Loss: {:.6f}'.format(epoch+1, train_loss_history[-1], val_loss_history[-1]))\n","\n","    return model, train_loss_history, val_loss_history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrPfrVOwQBJ9","outputId":"1899b9a9-c8af-4fdd-b8d7-0b480009d3df"},"outputs":[],"source":["train_datasets = []\n","val_datasets = []\n","untagged_ngrams = get_ngrams(dataset, show=True, max_features=10000)\n","for i in range(len(untagged_ngrams)):\n","    X_train, X_val, y_train, y_val = train_test_split(untagged_ngrams[i], dataset['label_bool'], test_size=0.2, random_state=42)\n","    train_datasets.append(TensorDataset(torch.from_numpy(X_train.toarray()).float(), torch.from_numpy(y_train.values).long()))\n","    val_datasets.append(TensorDataset(torch.from_numpy(X_val.toarray()).float(), torch.from_numpy(y_val.values).long()))\n","\n","train_loaders = []\n","val_loaders = []\n","for i in range(len(train_datasets)):\n","    train_loaders.append(DataLoader(train_datasets[i], batch_size=batch_size, shuffle=True))\n","    val_loaders.append(DataLoader(val_datasets[i], batch_size=batch_size, shuffle=False))\n","\n","print(train_loaders[0].dataset.tensors[0].shape)\n","print(val_loaders[0].dataset.tensors[0].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7ZLGQXeQBJ-","outputId":"a1ec1310-9108-4f66-8717-301e8d55269d"},"outputs":[],"source":["models = []\n","train_losses = []\n","val_losses = []\n","for i in range(len(train_loaders)):\n","    print(f'Ngram {i+1}:')\n","    model = Net(train_loaders[i].dataset.tensors[0].shape[1], 10, 1).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    _, train_loss_history, val_loss_history = train_model(model, criterion, optimizer,\n","                                                          train_loaders[i], val_loaders[i], epochs=epochs, device=device)\n","    models.append(model)\n","    train_losses.append(train_loss_history)\n","    val_losses.append(val_loss_history)\n","    print()\n","    print(\"-\"*100)\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-NWpsawQBJ-"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","for i in range(len(train_losses)):\n","    plt.plot(train_losses[i], label=f'ngram {i+1}')\n","    plt.plot(val_losses[i], label=f'ngram {i+1} val')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7lEDkoMQBJ-"},"outputs":[],"source":["# per model, run the val set and get accuracy, precision, recall, f1\n","\n","def get_metrics(model, val_loader, device='cpu'):\n","    model.eval()\n","    y_pred = []\n","    y_true = []\n","    with torch.no_grad():\n","        for X, y in val_loader:\n","            X = X.to(device)\n","            y = y.to(device).unsqueeze(1).to(torch.float32)\n","            y_pred.append(model(X.float()).squeeze(1).cpu().numpy())\n","            y_true.append(y.squeeze(1).cpu().numpy())\n","    y_pred = np.concatenate(y_pred)\n","    y_true = np.concatenate(y_true)\n","    y_pred = np.where(y_pred > 0.5, 1, 0)\n","    return accuracy_score(y_true, y_pred), precision_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7blj9rVQBJ_","outputId":"3fb257b1-013f-476c-f8ce-452e84015caa"},"outputs":[],"source":["metrics = []\n","for i in range(len(val_loaders)):\n","    accuracy, precision, recall, f1 = get_metrics(models[i], val_loaders[i], device=device)\n","    metrics.append([accuracy, precision, recall, f1, i+1])\n","metrics_df = pd.DataFrame(metrics, columns=['accuracy', 'precision', 'recall', 'f1', 'ngram'])\n","metrics_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvv3OVPHQBKA"},"outputs":[],"source":["metrics_df.to_csv(f'{results_dir}/pos-ngram-nn-results.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"yYMRSW-vQBKA"},"source":["## Error Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FhhzMZJQBKB"},"outputs":[],"source":["bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000)\n","bigrams = bigram_vectorizer.fit_transform(pos_tagged['text'])\n","classifier = MultinomialNB()\n","\n","X_train, X_test, y_train, y_test = train_test_split(bigrams, pos_tagged['label_bool'], test_size=0.2, random_state=42)\n","_, aux_X_test, _, _ = train_test_split(dataset['text'], dataset['label_bool'], test_size=0.2, random_state=42)\n","classifier.fit(X_train, y_train)\n","y_pred = classifier.predict(X_test)\n","# get wrong predictions and print text\n","wrong = np.where(y_pred != y_test)[0]\n","label_to_text = lambda x: 'PT' if x == 1 else 'BR'\n","missclassified = []\n","for i in wrong:\n","    # print example from aux_X_test\n","    print(f\"Predicted: {label_to_text(y_pred[i])} - True: {label_to_text(y_test.iloc[i])}\")\n","    print(aux_X_test.iloc[i])\n","    print()\n","    missclassified.append([aux_X_test.iloc[i], y_pred[i], y_test.iloc[i]])\n","\n","missclassified_df = pd.DataFrame(missclassified, columns=['text', 'predicted', 'true'])\n","missclassified_df.head(10, )"]}],"metadata":{"colab":{"collapsed_sections":["F2hck2GzQBJ7","yYMRSW-vQBKA"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
